{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/ham_spam.txt\", sep=\"\\t\", names=['label', 'message'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spam'] = pd.get_dummies(df['label'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message  spam\n",
       "0  Go until jurong point, crazy.. Available only ...     0\n",
       "1                      Ok lar... Joking wif u oni...     0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...     1\n",
       "3  U dun say so early hor... U c already then say...     0\n",
       "4  Nah I don't think he goes to usf, he lives aro...     0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['label'], inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['message'], df['spam'], test_size=0.3, random_state=45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1201             Otherwise had part time job na-tuition..\n",
       "77      I like you peoples very much:) but am very shy...\n",
       "1346      Que pases un buen tiempo or something like that\n",
       "2753                           Sat right? Okay thanks... \n",
       "3097    We walked from my moms. Right on stagwood pass...\n",
       "                              ...                        \n",
       "4473      3. You have received your mobile content. Enjoy\n",
       "580     Arngd marriage is while u r walkin unfortuntly...\n",
       "163     I'm so in love with you. I'm excited each day ...\n",
       "4703                                           Anytime...\n",
       "3616    I enjoy watching and playing football and bask...\n",
       "Name: message, Length: 3900, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import preprocessing\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class TextVectorization in module tensorflow.python.keras.layers.preprocessing.text_vectorization:\n",
      "\n",
      "class TextVectorization(tensorflow.python.keras.engine.base_preprocessing_layer.CombinerPreprocessingLayer)\n",
      " |  TextVectorization(*args, **kwargs)\n",
      " |  \n",
      " |  Text vectorization layer.\n",
      " |  \n",
      " |  This layer has basic options for managing text in a Keras model. It\n",
      " |  transforms a batch of strings (one sample = one string) into either a list of\n",
      " |  token indices (one sample = 1D tensor of integer token indices) or a dense\n",
      " |  representation (one sample = 1D tensor of float values representing data about\n",
      " |  the sample's tokens).\n",
      " |  \n",
      " |  If desired, the user can call this layer's adapt() method on a dataset.\n",
      " |  When this layer is adapted, it will analyze the dataset, determine the\n",
      " |  frequency of individual string values, and create a 'vocabulary' from them.\n",
      " |  This vocabulary can have unlimited size or be capped, depending on the\n",
      " |  configuration options for this layer; if there are more unique values in the\n",
      " |  input than the maximum vocabulary size, the most frequent terms will be used\n",
      " |  to create the vocabulary.\n",
      " |  \n",
      " |  The processing of each sample contains the following steps:\n",
      " |  \n",
      " |    1. standardize each sample (usually lowercasing + punctuation stripping)\n",
      " |    2. split each sample into substrings (usually words)\n",
      " |    3. recombine substrings into tokens (usually ngrams)\n",
      " |    4. index tokens (associate a unique int value with each token)\n",
      " |    5. transform each sample using this index, either into a vector of ints or\n",
      " |       a dense float vector.\n",
      " |  \n",
      " |  Some notes on passing Callables to customize splitting and normalization for\n",
      " |  this layer:\n",
      " |  \n",
      " |    1. Any callable can be passed to this Layer, but if you want to serialize\n",
      " |       this object you should only pass functions that are registered Keras\n",
      " |       serializables (see `tf.keras.utils.register_keras_serializable` for more\n",
      " |       details).\n",
      " |    2. When using a custom callable for `standardize`, the data received\n",
      " |       by the callable will be exactly as passed to this layer. The callable\n",
      " |       should return a tensor of the same shape as the input.\n",
      " |    3. When using a custom callable for `split`, the data received by the\n",
      " |       callable will have the 1st dimension squeezed out - instead of\n",
      " |       `[[\"string to split\"], [\"another string to split\"]]`, the Callable will\n",
      " |       see `[\"string to split\", \"another string to split\"]`. The callable should\n",
      " |       return a Tensor with the first dimension containing the split tokens -\n",
      " |       in this example, we should see something like `[[\"string\", \"to\", \"split],\n",
      " |       [\"another\", \"string\", \"to\", \"split\"]]`. This makes the callable site\n",
      " |       natively compatible with `tf.strings.split()`.\n",
      " |  \n",
      " |  Attributes:\n",
      " |    max_tokens: The maximum size of the vocabulary for this layer. If None,\n",
      " |      there is no cap on the size of the vocabulary. Note that this vocabulary\n",
      " |      contains 1 OOV token, so the effective number of tokens is `(max_tokens -\n",
      " |      1 - (1 if output == \"int\" else 0))`.\n",
      " |    standardize: Optional specification for standardization to apply to the\n",
      " |      input text. Values can be None (no standardization),\n",
      " |      'lower_and_strip_punctuation' (lowercase and remove punctuation) or a\n",
      " |      Callable. Default is 'lower_and_strip_punctuation'.\n",
      " |    split: Optional specification for splitting the input text. Values can be\n",
      " |      None (no splitting), 'whitespace' (split on ASCII whitespace), or a\n",
      " |      Callable. The default is 'whitespace'.\n",
      " |    ngrams: Optional specification for ngrams to create from the possibly-split\n",
      " |      input text. Values can be None, an integer or tuple of integers; passing\n",
      " |      an integer will create ngrams up to that integer, and passing a tuple of\n",
      " |      integers will create ngrams for the specified values in the tuple. Passing\n",
      " |      None means that no ngrams will be created.\n",
      " |    output_mode: Optional specification for the output of the layer. Values can\n",
      " |      be \"int\", \"binary\", \"count\" or \"tf-idf\", configuring the layer as follows:\n",
      " |        \"int\": Outputs integer indices, one integer index per split string\n",
      " |          token. When output == \"int\", 0 is reserved for masked locations;\n",
      " |          this reduces the vocab size to max_tokens-2 instead of max_tokens-1\n",
      " |        \"binary\": Outputs a single int array per batch, of either vocab_size or\n",
      " |          max_tokens size, containing 1s in all elements where the token mapped\n",
      " |          to that index exists at least once in the batch item.\n",
      " |        \"count\": As \"binary\", but the int array contains a count of the number\n",
      " |          of times the token at that index appeared in the batch item.\n",
      " |        \"tf-idf\": As \"binary\", but the TF-IDF algorithm is applied to find the\n",
      " |          value in each token slot.\n",
      " |    output_sequence_length: Only valid in INT mode. If set, the output will have\n",
      " |      its time dimension padded or truncated to exactly `output_sequence_length`\n",
      " |      values, resulting in a tensor of shape [batch_size,\n",
      " |      output_sequence_length] regardless of how many tokens resulted from the\n",
      " |      splitting step. Defaults to None.\n",
      " |    pad_to_max_tokens: Only valid in  \"binary\", \"count\", and \"tf-idf\" modes. If\n",
      " |      True, the output will have its feature axis padded to `max_tokens` even if\n",
      " |      the number of unique tokens in the vocabulary is less than max_tokens,\n",
      " |      resulting in a tensor of shape [batch_size, max_tokens] regardless of\n",
      " |      vocabulary size. Defaults to True.\n",
      " |  \n",
      " |  Example:\n",
      " |  This example instantiates a TextVectorization layer that lowercases text,\n",
      " |  splits on whitespace, strips punctuation, and outputs integer vocab indices.\n",
      " |  \n",
      " |  >>> text_dataset = tf.data.Dataset.from_tensor_slices([\"foo\", \"bar\", \"baz\"])\n",
      " |  >>> max_features = 5000  # Maximum vocab size.\n",
      " |  >>> max_len = 4  # Sequence length to pad the outputs to.\n",
      " |  >>> embedding_dims = 2\n",
      " |  >>>\n",
      " |  >>> # Create the layer.\n",
      " |  >>> vectorize_layer = TextVectorization(\n",
      " |  ...  max_tokens=max_features,\n",
      " |  ...  output_mode='int',\n",
      " |  ...  output_sequence_length=max_len)\n",
      " |  >>>\n",
      " |  >>> # Now that the vocab layer has been created, call `adapt` on the text-only\n",
      " |  >>> # dataset to create the vocabulary. You don't have to batch, but for large\n",
      " |  >>> # datasets this means we're not keeping spare copies of the dataset.\n",
      " |  >>> vectorize_layer.adapt(text_dataset.batch(64))\n",
      " |  >>>\n",
      " |  >>> # Create the model that uses the vectorize text layer\n",
      " |  >>> model = tf.keras.models.Sequential()\n",
      " |  >>>\n",
      " |  >>> # Start by creating an explicit input layer. It needs to have a shape of\n",
      " |  >>> # (1,) (because we need to guarantee that there is exactly one string\n",
      " |  >>> # input per batch), and the dtype needs to be 'string'.\n",
      " |  >>> model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
      " |  >>>\n",
      " |  >>> # The first layer in our model is the vectorization layer. After this\n",
      " |  >>> # layer, we have a tensor of shape (batch_size, max_len) containing vocab\n",
      " |  >>> # indices.\n",
      " |  >>> model.add(vectorize_layer)\n",
      " |  >>>\n",
      " |  >>> # Now, the model can map strings to integers, and you can add an embedding\n",
      " |  >>> # layer to map these integers to learned embeddings.\n",
      " |  >>> input_data = [[\"foo qux bar\"], [\"qux baz\"]]\n",
      " |  >>> model.predict(input_data)\n",
      " |  array([[2, 1, 4, 0],\n",
      " |         [1, 3, 0, 0]])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      TextVectorization\n",
      " |      tensorflow.python.keras.engine.base_preprocessing_layer.CombinerPreprocessingLayer\n",
      " |      tensorflow.python.keras.engine.base_preprocessing_layer.PreprocessingLayer\n",
      " |      tensorflow.python.keras.engine.base_layer.Layer\n",
      " |      tensorflow.python.module.module.Module\n",
      " |      tensorflow.python.training.tracking.tracking.AutoTrackable\n",
      " |      tensorflow.python.training.tracking.base.Trackable\n",
      " |      tensorflow.python.keras.utils.version_utils.LayerVersionSelector\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, max_tokens=None, standardize='lower_and_strip_punctuation', split='whitespace', ngrams=None, output_mode='int', output_sequence_length=None, pad_to_max_tokens=True, **kwargs)\n",
      " |  \n",
      " |  adapt(self, data, reset_state=True)\n",
      " |      Fits the state of the preprocessing layer to the dataset.\n",
      " |      \n",
      " |      Overrides the default adapt method to apply relevant preprocessing to the\n",
      " |      inputs before passing to the combiner.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        data: The data to train on. It can be passed either as a tf.data Dataset,\n",
      " |          as a NumPy array, a string tensor, or as a list of texts.\n",
      " |        reset_state: Optional argument specifying whether to clear the state of\n",
      " |          the layer at the start of the call to `adapt`. This must be True for\n",
      " |          this layer, which does not support repeated calls to `adapt`.\n",
      " |  \n",
      " |  build(self, input_shape)\n",
      " |      Creates the variables of the layer (optional, for subclass implementers).\n",
      " |      \n",
      " |      This is a method that implementers of subclasses of `Layer` or `Model`\n",
      " |      can override if they need a state-creation step in-between\n",
      " |      layer instantiation and layer call.\n",
      " |      \n",
      " |      This is typically used to create the weights of `Layer` subclasses.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        input_shape: Instance of `TensorShape`, or list of instances of\n",
      " |          `TensorShape` if the layer expects a list of inputs\n",
      " |          (one instance per input).\n",
      " |  \n",
      " |  call(self, inputs)\n",
      " |      This is where the layer's logic lives.\n",
      " |      \n",
      " |      Note here that `call()` method in `tf.keras` is little bit different\n",
      " |      from `keras` API. In `keras` API, you can pass support masking for\n",
      " |      layers as additional arguments. Whereas `tf.keras` has `compute_mask()`\n",
      " |      method to support masking.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Input tensor, or list/tuple of input tensors.\n",
      " |          **kwargs: Additional keyword arguments. Currently unused.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor or list/tuple of tensors.\n",
      " |  \n",
      " |  compute_output_shape(self, input_shape)\n",
      " |      Computes the output shape of the layer.\n",
      " |      \n",
      " |      If the layer has not been built, this method will call `build` on the\n",
      " |      layer. This assumes that the layer will later be used with inputs that\n",
      " |      match the input shape provided here.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          input_shape: Shape tuple (tuple of integers)\n",
      " |              or list of shape tuples (one per output tensor of the layer).\n",
      " |              Shape tuples can include None for free dimensions,\n",
      " |              instead of an integer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An input shape tuple.\n",
      " |  \n",
      " |  compute_output_signature(self, input_spec)\n",
      " |      Compute the output tensor signature of the layer based on the inputs.\n",
      " |      \n",
      " |      Unlike a TensorShape object, a TensorSpec object contains both shape\n",
      " |      and dtype information for a tensor. This method allows layers to provide\n",
      " |      output dtype information if it is different from the input dtype.\n",
      " |      For any layer that doesn't implement this function,\n",
      " |      the framework will fall back to use `compute_output_shape`, and will\n",
      " |      assume that the output dtype matches the input dtype.\n",
      " |      \n",
      " |      Args:\n",
      " |        input_signature: Single TensorSpec or nested structure of TensorSpec\n",
      " |          objects, describing a candidate input for the layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Single TensorSpec or nested structure of TensorSpec objects, describing\n",
      " |          how the layer would transform the provided input.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If input_signature contains a non-TensorSpec object.\n",
      " |  \n",
      " |  count_params(self)\n",
      " |      Count the total number of scalars composing the weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |          An integer count.\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: if the layer isn't yet built\n",
      " |            (in which case its weights aren't yet defined).\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Returns the config of the layer.\n",
      " |      \n",
      " |      A layer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of a layer.\n",
      " |      The same layer can be reinstantiated later\n",
      " |      (without its trained weights) from this configuration.\n",
      " |      \n",
      " |      The config of a layer does not include connectivity\n",
      " |      information, nor the layer class name. These are handled\n",
      " |      by `Network` (one layer of abstraction above).\n",
      " |      \n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |  \n",
      " |  get_vocabulary(self)\n",
      " |  \n",
      " |  set_vocabulary(self, vocab, df_data=None, oov_df_value=None)\n",
      " |      Sets vocabulary (and optionally document frequency) data for this layer.\n",
      " |      \n",
      " |      This method sets the vocabulary and DF data for this layer directly, instead\n",
      " |      of analyzing a dataset through 'adapt'. It should be used whenever the vocab\n",
      " |      (and optionally document frequency) information is already known. If\n",
      " |      vocabulary data is already present in the layer, this method will replace\n",
      " |      it.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        vocab: An array of string tokens.\n",
      " |        df_data: An array of document frequency data. Only necessary if the layer\n",
      " |          output_mode is TFIDF.\n",
      " |        oov_df_value: The document frequency of the OOV token. Only necessary if\n",
      " |          output_mode is TFIDF.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If there are too many inputs, the inputs do not match, or\n",
      " |          input data is missing.\n",
      " |        RuntimeError: If the vocabulary cannot be set when this function is\n",
      " |          called. This happens when \"binary\", \"count\", and \"tfidf\" modes,\n",
      " |          if \"pad_to_max_tokens\" is False and the layer itself has already been\n",
      " |          called.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from tensorflow.python.keras.engine.base_preprocessing_layer.PreprocessingLayer:\n",
      " |  \n",
      " |  __metaclass__ = <class 'abc.ABCMeta'>\n",
      " |      Metaclass for defining Abstract Base Classes (ABCs).\n",
      " |      \n",
      " |      Use this metaclass to create an ABC.  An ABC can be subclassed\n",
      " |      directly, and then acts as a mix-in class.  You can also register\n",
      " |      unrelated concrete classes (even built-in classes) and unrelated\n",
      " |      ABCs as 'virtual subclasses' -- these and their descendants will\n",
      " |      be considered subclasses of the registering ABC by the built-in\n",
      " |      issubclass() function, but the registering ABC won't show up in\n",
      " |      their MRO (Method Resolution Order) nor will method\n",
      " |      implementations defined by the registering ABC be callable (not\n",
      " |      even via super()).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  __call__(self, *args, **kwargs)\n",
      " |      Wraps `call`, applying pre- and post-processing steps.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        *args: Positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: Keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |      \n",
      " |      Note:\n",
      " |        - The following optional keyword arguments are reserved for specific uses:\n",
      " |          * `training`: Boolean scalar tensor of Python boolean indicating\n",
      " |            whether the `call` is meant for training or inference.\n",
      " |          * `mask`: Boolean input mask.\n",
      " |        - If the layer's `call` method takes a `mask` argument (as some Keras\n",
      " |          layers do), its default value will be set to the mask generated\n",
      " |          for `inputs` by the previous layer (if `input` did come from\n",
      " |          a layer that generated a corresponding mask, i.e. if it came from\n",
      " |          a Keras layer with masking support.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if the layer's `call` method returns None (an invalid value).\n",
      " |        RuntimeError: if `super().__init__()` was not called in the constructor.\n",
      " |  \n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  add_loss(self, losses, **kwargs)\n",
      " |      Add loss tensor(s), potentially dependent on layer inputs.\n",
      " |      \n",
      " |      Some losses (for instance, activity regularization losses) may be dependent\n",
      " |      on the inputs passed when calling a layer. Hence, when reusing the same\n",
      " |      layer on different inputs `a` and `b`, some entries in `layer.losses` may\n",
      " |      be dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This method can be used inside a subclassed layer or model's `call`\n",
      " |      function, in which case `losses` should be a Tensor or list of Tensors.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyLayer(tf.keras.layers.Layer):\n",
      " |        def call(self, inputs):\n",
      " |          self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any loss Tensors passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      losses become part of the model's topology and are tracked in `get_config`.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Activity regularization.\n",
      " |      model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      ```\n",
      " |      \n",
      " |      If this is not the case for your loss (if, for example, your loss references\n",
      " |      a `Variable` of one of the model's layers), you can wrap your loss in a\n",
      " |      zero-argument lambda. These losses are not tracked as part of the model's\n",
      " |      topology since they can't be serialized.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      d = tf.keras.layers.Dense(10)\n",
      " |      x = d(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      # Weight regularization.\n",
      " |      model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      ```\n",
      " |      \n",
      " |      Arguments:\n",
      " |        losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses\n",
      " |          may also be zero-argument callables which create a loss tensor.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |            inputs - Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_metric(self, value, name=None, **kwargs)\n",
      " |      Adds metric tensor to the layer.\n",
      " |      \n",
      " |      This method can be used inside the `call()` method of a subclassed layer\n",
      " |      or model.\n",
      " |      \n",
      " |      ```python\n",
      " |      class MyMetricLayer(tf.keras.layers.Layer):\n",
      " |        def __init__(self):\n",
      " |          super(MyMetricLayer, self).__init__(name='my_metric_layer')\n",
      " |          self.mean = metrics_module.Mean(name='metric_1')\n",
      " |      \n",
      " |        def call(self, inputs):\n",
      " |          self.add_metric(self.mean(x))\n",
      " |          self.add_metric(math_ops.reduce_sum(x), name='metric_2')\n",
      " |          return inputs\n",
      " |      ```\n",
      " |      \n",
      " |      This method can also be called directly on a Functional Model during\n",
      " |      construction. In this case, any tensor passed to this Model must\n",
      " |      be symbolic and be able to be traced back to the model's `Input`s. These\n",
      " |      metrics become part of the model's topology and are tracked when you\n",
      " |      save the model via `save()`.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(math_ops.reduce_sum(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Note: Calling `add_metric()` with the result of a metric object on a\n",
      " |      Functional Model, as shown in the example below, is not supported. This is\n",
      " |      because we cannot trace the metric result tensor back to the model's inputs.\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.keras.Input(shape=(10,))\n",
      " |      x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      model = tf.keras.Model(inputs, outputs)\n",
      " |      model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1')\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        value: Metric tensor.\n",
      " |        name: String metric name.\n",
      " |        **kwargs: Additional keyword arguments for backward compatibility.\n",
      " |          Accepted values:\n",
      " |          `aggregation` - When the `value` tensor provided is not the result of\n",
      " |          calling a `keras.Metric` instance, it will be aggregated by default\n",
      " |          using a `keras.Metric.Mean`.\n",
      " |  \n",
      " |  add_update(self, updates, inputs=None)\n",
      " |      Add update op(s), potentially dependent on layer inputs. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(inputs)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      `inputs` is now automatically inferred\n",
      " |      \n",
      " |      Weight updates (for instance, the updates of the moving mean and variance\n",
      " |      in a BatchNormalization layer) may be dependent on the inputs passed\n",
      " |      when calling a layer. Hence, when reusing the same layer on\n",
      " |      different inputs `a` and `b`, some entries in `layer.updates` may be\n",
      " |      dependent on `a` and some on `b`. This method automatically keeps track\n",
      " |      of dependencies.\n",
      " |      \n",
      " |      This call is ignored when eager execution is enabled (in that case, variable\n",
      " |      updates are run on the fly and thus do not need to be tracked for later\n",
      " |      execution).\n",
      " |      \n",
      " |      Arguments:\n",
      " |        updates: Update op, or list/tuple of update ops, or zero-arg callable\n",
      " |          that returns an update op. A zero-arg callable should be passed in\n",
      " |          order to disable running the updates by setting `trainable=False`\n",
      " |          on this Layer, when executing in Eager mode.\n",
      " |        inputs: Deprecated, will be automatically inferred.\n",
      " |  \n",
      " |  add_variable(self, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! Alias for `add_weight`. (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.add_weight` method instead.\n",
      " |  \n",
      " |  add_weight(self, name=None, shape=None, dtype=None, initializer=None, regularizer=None, trainable=None, constraint=None, partitioner=None, use_resource=None, synchronization=<VariableSynchronization.AUTO: 0>, aggregation=<VariableAggregation.NONE: 0>, **kwargs)\n",
      " |      Adds a new variable to the layer.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        name: Variable name.\n",
      " |        shape: Variable shape. Defaults to scalar if unspecified.\n",
      " |        dtype: The type of the variable. Defaults to `self.dtype` or `float32`.\n",
      " |        initializer: Initializer instance (callable).\n",
      " |        regularizer: Regularizer instance (callable).\n",
      " |        trainable: Boolean, whether the variable should be part of the layer's\n",
      " |          \"trainable_variables\" (e.g. variables, biases)\n",
      " |          or \"non_trainable_variables\" (e.g. BatchNorm mean and variance).\n",
      " |          Note that `trainable` cannot be `True` if `synchronization`\n",
      " |          is set to `ON_READ`.\n",
      " |        constraint: Constraint instance (callable).\n",
      " |        partitioner: Partitioner to be passed to the `Trackable` API.\n",
      " |        use_resource: Whether to use `ResourceVariable`.\n",
      " |        synchronization: Indicates when a distributed a variable will be\n",
      " |          aggregated. Accepted values are constants defined in the class\n",
      " |          `tf.VariableSynchronization`. By default the synchronization is set to\n",
      " |          `AUTO` and the current `DistributionStrategy` chooses\n",
      " |          when to synchronize. If `synchronization` is set to `ON_READ`,\n",
      " |          `trainable` must not be set to `True`.\n",
      " |        aggregation: Indicates how a distributed variable will be aggregated.\n",
      " |          Accepted values are constants defined in the class\n",
      " |          `tf.VariableAggregation`.\n",
      " |        **kwargs: Additional keyword arguments. Accepted values are `getter`,\n",
      " |          `collections`, `experimental_autocast` and `caching_device`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The created variable. Usually either a `Variable` or `ResourceVariable`\n",
      " |        instance. If `partitioner` is not `None`, a `PartitionedVariable`\n",
      " |        instance is returned.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called with partitioned variable regularization and\n",
      " |          eager execution is enabled.\n",
      " |        ValueError: When giving unsupported dtype and no initializer or when\n",
      " |          trainable has been set to True with synchronization set as `ON_READ`.\n",
      " |  \n",
      " |  apply(self, inputs, *args, **kwargs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.__call__` method instead.\n",
      " |      \n",
      " |      This is an alias of `self.__call__`.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor(s).\n",
      " |        *args: additional positional arguments to be passed to `self.call`.\n",
      " |        **kwargs: additional keyword arguments to be passed to `self.call`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor(s).\n",
      " |  \n",
      " |  compute_mask(self, inputs, mask=None)\n",
      " |      Computes an output mask tensor.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          inputs: Tensor or list of tensors.\n",
      " |          mask: Tensor or list of tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |          None or a tensor (or list of tensors,\n",
      " |              one per output tensor of the layer).\n",
      " |  \n",
      " |  get_input_at(self, node_index)\n",
      " |      Retrieves the input tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_input_mask_at(self, node_index)\n",
      " |      Retrieves the input mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple inputs).\n",
      " |  \n",
      " |  get_input_shape_at(self, node_index)\n",
      " |      Retrieves the input shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple inputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_losses_for(self, inputs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.losses` instead.\n",
      " |      \n",
      " |      Retrieves losses relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of loss tensors of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_output_at(self, node_index)\n",
      " |      Retrieves the output tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A tensor (or list of tensors if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_output_mask_at(self, node_index)\n",
      " |      Retrieves the output mask tensor(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A mask tensor\n",
      " |          (or list of tensors if the layer has multiple outputs).\n",
      " |  \n",
      " |  get_output_shape_at(self, node_index)\n",
      " |      Retrieves the output shape(s) of a layer at a given node.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          node_index: Integer, index of the node\n",
      " |              from which to retrieve the attribute.\n",
      " |              E.g. `node_index=0` will correspond to the\n",
      " |              first time the layer was called.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A shape tuple\n",
      " |          (or list of shape tuples if the layer has multiple outputs).\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |  \n",
      " |  get_updates_for(self, inputs)\n",
      " |      Deprecated, do NOT use! (deprecated)\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Please use `layer.updates` instead.\n",
      " |      \n",
      " |      Retrieves updates relevant to a specific set of inputs.\n",
      " |      \n",
      " |      Arguments:\n",
      " |        inputs: Input tensor or list/tuple of input tensors.\n",
      " |      \n",
      " |      Returns:\n",
      " |        List of update ops of the layer that depend on `inputs`.\n",
      " |  \n",
      " |  get_weights(self)\n",
      " |      Returns the current weights of the layer.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      returns both trainable and non-trainable weight values associated with this\n",
      " |      layer as a list of Numpy arrays, which can in turn be used to load state\n",
      " |      into similarly parameterized layers.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Returns:\n",
      " |          Weights values as a list of numpy arrays.\n",
      " |  \n",
      " |  set_weights(self, weights)\n",
      " |      Sets the weights of the layer, from Numpy arrays.\n",
      " |      \n",
      " |      The weights of a layer represent the state of the layer. This function\n",
      " |      sets the weight values from numpy arrays. The weight values should be\n",
      " |      passed in the order they are created by the layer. Note that the layer's\n",
      " |      weights must be instantiated before calling this function by calling\n",
      " |      the layer.\n",
      " |      \n",
      " |      For example, a Dense layer returns a list of two values-- per-output\n",
      " |      weights and the bias value. These can be used to set the weights of another\n",
      " |      Dense layer:\n",
      " |      \n",
      " |      >>> a = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(1.))\n",
      " |      >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))\n",
      " |      >>> a.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b = tf.keras.layers.Dense(1,\n",
      " |      ...   kernel_initializer=tf.constant_initializer(2.))\n",
      " |      >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[2.],\n",
      " |             [2.],\n",
      " |             [2.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      >>> b.set_weights(a.get_weights())\n",
      " |      >>> b.get_weights()\n",
      " |      [array([[1.],\n",
      " |             [1.],\n",
      " |             [1.]], dtype=float32), array([0.], dtype=float32)]\n",
      " |      \n",
      " |      Arguments:\n",
      " |          weights: a list of Numpy arrays. The number\n",
      " |              of arrays and their shape must match\n",
      " |              number of the dimensions of the weights\n",
      " |              of the layer (i.e. it should match the\n",
      " |              output of `get_weights`).\n",
      " |      \n",
      " |      Raises:\n",
      " |          ValueError: If the provided weights list does not match the\n",
      " |              layer's specifications.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  from_config(config) from builtins.type\n",
      " |      Creates a layer from its config.\n",
      " |      \n",
      " |      This method is the reverse of `get_config`,\n",
      " |      capable of instantiating the same layer from the config\n",
      " |      dictionary. It does not handle layer connectivity\n",
      " |      (handled by Network), nor weights (handled by `set_weights`).\n",
      " |      \n",
      " |      Arguments:\n",
      " |          config: A Python dictionary, typically the\n",
      " |              output of get_config.\n",
      " |      \n",
      " |      Returns:\n",
      " |          A layer instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  dtype\n",
      " |      Dtype used by the weights of the layer, set in the constructor.\n",
      " |  \n",
      " |  dynamic\n",
      " |      Whether the layer is dynamic (eager-only); set in the constructor.\n",
      " |  \n",
      " |  inbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  input\n",
      " |      Retrieves the input tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input tensor or list of input tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If called in Eager mode.\n",
      " |        AttributeError: If no inbound nodes are found.\n",
      " |  \n",
      " |  input_mask\n",
      " |      Retrieves the input mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input mask tensor (potentially None) or list of input\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  input_shape\n",
      " |      Retrieves the input shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one input,\n",
      " |      i.e. if it is connected to one incoming layer, or if all inputs\n",
      " |      have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Input shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per input tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined input_shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  losses\n",
      " |      List of losses added using the `add_loss()` API.\n",
      " |      \n",
      " |      Variable regularization tensors are created when this property is accessed,\n",
      " |      so it is eager safe: accessing `losses` under a `tf.GradientTape` will\n",
      " |      propagate gradients back to the corresponding variables.\n",
      " |      \n",
      " |      Examples:\n",
      " |      \n",
      " |      >>> class MyLayer(tf.keras.layers.Layer):\n",
      " |      ...   def call(self, inputs):\n",
      " |      ...     self.add_loss(tf.abs(tf.reduce_mean(inputs)))\n",
      " |      ...     return inputs\n",
      " |      >>> l = MyLayer()\n",
      " |      >>> l(np.ones((10, 1)))\n",
      " |      >>> l.losses\n",
      " |      [1.0]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> x = tf.keras.layers.Dense(10)(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Activity regularization.\n",
      " |      >>> model.add_loss(tf.abs(tf.reduce_mean(x)))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor 'Abs:0' shape=() dtype=float32>]\n",
      " |      \n",
      " |      >>> inputs = tf.keras.Input(shape=(10,))\n",
      " |      >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones')\n",
      " |      >>> x = d(inputs)\n",
      " |      >>> outputs = tf.keras.layers.Dense(1)(x)\n",
      " |      >>> model = tf.keras.Model(inputs, outputs)\n",
      " |      >>> # Weight regularization.\n",
      " |      >>> model.add_loss(lambda: tf.reduce_mean(d.kernel))\n",
      " |      >>> model.losses\n",
      " |      [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  metrics\n",
      " |      List of metrics added using the `add_metric()` API.\n",
      " |      \n",
      " |      Example:\n",
      " |      \n",
      " |      >>> input = tf.keras.layers.Input(shape=(3,))\n",
      " |      >>> d = tf.keras.layers.Dense(2)\n",
      " |      >>> output = d(input)\n",
      " |      >>> d.add_metric(tf.reduce_max(output), name='max')\n",
      " |      >>> d.add_metric(tf.reduce_min(output), name='min')\n",
      " |      >>> [m.name for m in d.metrics]\n",
      " |      ['max', 'min']\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of tensors.\n",
      " |  \n",
      " |  name\n",
      " |      Name of the layer (string), set in the constructor.\n",
      " |  \n",
      " |  non_trainable_variables\n",
      " |  \n",
      " |  non_trainable_weights\n",
      " |      List of all non-trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Non-trainable weights are *not* updated during training. They are expected\n",
      " |      to be updated manually in `call()`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of non-trainable variables.\n",
      " |  \n",
      " |  outbound_nodes\n",
      " |      Deprecated, do NOT use! Only for compatibility with external Keras.\n",
      " |  \n",
      " |  output\n",
      " |      Retrieves the output tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one output,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |        Output tensor or list of output tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |        AttributeError: if the layer is connected to more than one incoming\n",
      " |          layers.\n",
      " |        RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  output_mask\n",
      " |      Retrieves the output mask tensor(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has exactly one inbound node,\n",
      " |      i.e. if it is connected to one incoming layer.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output mask tensor (potentially None) or list of output\n",
      " |          mask tensors.\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer is connected to\n",
      " |          more than one incoming layers.\n",
      " |  \n",
      " |  output_shape\n",
      " |      Retrieves the output shape(s) of a layer.\n",
      " |      \n",
      " |      Only applicable if the layer has one output,\n",
      " |      or if all outputs have the same shape.\n",
      " |      \n",
      " |      Returns:\n",
      " |          Output shape, as an integer shape tuple\n",
      " |          (or list of shape tuples, one tuple per output tensor).\n",
      " |      \n",
      " |      Raises:\n",
      " |          AttributeError: if the layer has no defined output shape.\n",
      " |          RuntimeError: if called in Eager mode.\n",
      " |  \n",
      " |  trainable_variables\n",
      " |      Sequence of trainable variables owned by this module and its submodules.\n",
      " |      \n",
      " |      Note: this method uses reflection to find variables on the current instance\n",
      " |      and submodules. For performance reasons you may wish to cache the result\n",
      " |      of calling this method if you don't expect the return value to change.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of variables for the current module (sorted by attribute\n",
      " |        name) followed by variables from all submodules recursively (breadth\n",
      " |        first).\n",
      " |  \n",
      " |  trainable_weights\n",
      " |      List of all trainable weights tracked by this layer.\n",
      " |      \n",
      " |      Trainable weights are updated via gradient descent during training.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of trainable variables.\n",
      " |  \n",
      " |  updates\n",
      " |      DEPRECATED FUNCTION\n",
      " |      \n",
      " |      Warning: THIS FUNCTION IS DEPRECATED. It will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      " |  \n",
      " |  variables\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Alias of `self.weights`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  weights\n",
      " |      Returns the list of all layer variables/weights.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of variables.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.keras.engine.base_layer.Layer:\n",
      " |  \n",
      " |  activity_regularizer\n",
      " |      Optional regularizer function for the output of this layer.\n",
      " |  \n",
      " |  input_spec\n",
      " |      `InputSpec` instance(s) describing the input format for this layer.\n",
      " |      \n",
      " |      When you create a layer subclass, you can set `self.input_spec` to enable\n",
      " |      the layer to run input compatibility checks when it is called.\n",
      " |      Consider a `Conv2D` layer: it can only be called on a single input tensor\n",
      " |      of rank 4. As such, you can set, in `__init__()`:\n",
      " |      \n",
      " |      ```python\n",
      " |      self.input_spec = tf.keras.layers.InputSpec(ndim=4)\n",
      " |      ```\n",
      " |      \n",
      " |      Now, if you try to call the layer on an input that isn't rank 4\n",
      " |      (for instance, an input of shape `(2,)`, it will raise a nicely-formatted\n",
      " |      error:\n",
      " |      \n",
      " |      ```\n",
      " |      ValueError: Input 0 of layer conv2d is incompatible with the layer:\n",
      " |      expected ndim=4, found ndim=1. Full shape received: [2]\n",
      " |      ```\n",
      " |      \n",
      " |      Input checks that can be specified via `input_spec` include:\n",
      " |      - Structure (e.g. a single input, a list of 2 inputs, etc)\n",
      " |      - Shape\n",
      " |      - Rank (ndim)\n",
      " |      - Dtype\n",
      " |      \n",
      " |      For more information, see `tf.keras.layers.InputSpec`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `tf.keras.layers.InputSpec` instance, or nested structure thereof.\n",
      " |  \n",
      " |  stateful\n",
      " |  \n",
      " |  supports_masking\n",
      " |      Whether this layer supports computing a mask using `compute_mask`.\n",
      " |  \n",
      " |  trainable\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  with_name_scope(method) from builtins.type\n",
      " |      Decorator to automatically enter the module name scope.\n",
      " |      \n",
      " |      >>> class MyModule(tf.Module):\n",
      " |      ...   @tf.Module.with_name_scope\n",
      " |      ...   def __call__(self, x):\n",
      " |      ...     if not hasattr(self, 'w'):\n",
      " |      ...       self.w = tf.Variable(tf.random.normal([x.shape[1], 3]))\n",
      " |      ...     return tf.matmul(x, self.w)\n",
      " |      \n",
      " |      Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose\n",
      " |      names included the module name:\n",
      " |      \n",
      " |      >>> mod = MyModule()\n",
      " |      >>> mod(tf.ones([1, 2]))\n",
      " |      <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)>\n",
      " |      >>> mod.w\n",
      " |      <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32,\n",
      " |      numpy=..., dtype=float32)>\n",
      " |      \n",
      " |      Args:\n",
      " |        method: The method to wrap.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The original method wrapped such that it enters the module's name scope.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from tensorflow.python.module.module.Module:\n",
      " |  \n",
      " |  name_scope\n",
      " |      Returns a `tf.name_scope` instance for this class.\n",
      " |  \n",
      " |  submodules\n",
      " |      Sequence of all sub-modules.\n",
      " |      \n",
      " |      Submodules are modules which are properties of this module, or found as\n",
      " |      properties of modules which are properties of this module (and so on).\n",
      " |      \n",
      " |      >>> a = tf.Module()\n",
      " |      >>> b = tf.Module()\n",
      " |      >>> c = tf.Module()\n",
      " |      >>> a.b = b\n",
      " |      >>> b.c = c\n",
      " |      >>> list(a.submodules) == [b, c]\n",
      " |      True\n",
      " |      >>> list(b.submodules) == [c]\n",
      " |      True\n",
      " |      >>> list(c.submodules) == []\n",
      " |      True\n",
      " |      \n",
      " |      Returns:\n",
      " |        A sequence of all submodules.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.training.tracking.base.Trackable:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods inherited from tensorflow.python.keras.utils.version_utils.LayerVersionSelector:\n",
      " |  \n",
      " |  __new__(cls, *args, **kwargs)\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(TextVectorization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization_layer = TextVectorization(\n",
    "    max_tokens=None, standardize=\"lower_and_strip_punctuation\",\n",
    "    split=\"whitespace\", ngrams=None, output_mode=\"int\",\n",
    "    output_sequence_length=None, pad_to_max_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorization_layer.adapt(X_train.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1201 --->  custcare\n",
      " 313 --->  into\n",
      "Vocabulary size: 7922\n"
     ]
    }
   ],
   "source": [
    "print(\"1201 ---> \",vectorization_layer.get_vocabulary()[1201])\n",
    "print(\" 313 ---> \",vectorization_layer.get_vocabulary()[313])\n",
    "print('Vocabulary size: {}'.format(len(vectorization_layer.get_vocabulary())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "text_vectorization_2 (TextVe (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, None, 16)          126768    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, None, 16)          0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 17        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 126,787\n",
      "Trainable params: 126,787\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation,Dropout\n",
    "\n",
    "embedding_dim = 16\n",
    "max_features = 7922\n",
    "\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.Input(shape=(1,), dtype=tf.string))\n",
    "\n",
    "model.add(vectorization_layer)\n",
    "\n",
    "model.add(layers.Embedding(max_features + 1, embedding_dim))\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.GlobalAveragePooling1D())\n",
    "model.add(layers.Dropout(0.2))\n",
    "\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=tf.metrics.BinaryAccuracy(threshold=0.0))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.6518 - binary_accuracy: 0.1341 - val_loss: 0.5800 - val_binary_accuracy: 0.1340\n",
      "Epoch 2/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.4670 - binary_accuracy: 0.1341 - val_loss: 0.3690 - val_binary_accuracy: 0.1340\n",
      "Epoch 3/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.3289 - binary_accuracy: 0.1341 - val_loss: 0.3094 - val_binary_accuracy: 0.1340\n",
      "Epoch 4/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.2892 - binary_accuracy: 0.1341 - val_loss: 0.2898 - val_binary_accuracy: 0.1340\n",
      "Epoch 5/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.2668 - binary_accuracy: 0.1341 - val_loss: 0.2699 - val_binary_accuracy: 0.1340\n",
      "Epoch 6/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.2410 - binary_accuracy: 0.1341 - val_loss: 0.2486 - val_binary_accuracy: 0.1340\n",
      "Epoch 7/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.2235 - binary_accuracy: 0.1341 - val_loss: 0.2242 - val_binary_accuracy: 0.1340\n",
      "Epoch 8/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1897 - binary_accuracy: 0.1341 - val_loss: 0.2000 - val_binary_accuracy: 0.1340\n",
      "Epoch 9/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1624 - binary_accuracy: 0.1341 - val_loss: 0.1769 - val_binary_accuracy: 0.1340\n",
      "Epoch 10/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1443 - binary_accuracy: 0.1341 - val_loss: 0.1562 - val_binary_accuracy: 0.1340\n",
      "Epoch 11/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1196 - binary_accuracy: 0.1341 - val_loss: 0.1392 - val_binary_accuracy: 0.1340\n",
      "Epoch 12/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.1062 - binary_accuracy: 0.1341 - val_loss: 0.1253 - val_binary_accuracy: 0.1340\n",
      "Epoch 13/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0913 - binary_accuracy: 0.1341 - val_loss: 0.1152 - val_binary_accuracy: 0.1340\n",
      "Epoch 14/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0844 - binary_accuracy: 0.1341 - val_loss: 0.1064 - val_binary_accuracy: 0.1340\n",
      "Epoch 15/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0740 - binary_accuracy: 0.1341 - val_loss: 0.0999 - val_binary_accuracy: 0.1340\n",
      "Epoch 16/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0712 - binary_accuracy: 0.1341 - val_loss: 0.0927 - val_binary_accuracy: 0.1340\n",
      "Epoch 17/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0592 - binary_accuracy: 0.1341 - val_loss: 0.0884 - val_binary_accuracy: 0.1340\n",
      "Epoch 18/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0577 - binary_accuracy: 0.1341 - val_loss: 0.0845 - val_binary_accuracy: 0.1340\n",
      "Epoch 19/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0539 - binary_accuracy: 0.1341 - val_loss: 0.0808 - val_binary_accuracy: 0.1340\n",
      "Epoch 20/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0470 - binary_accuracy: 0.1341 - val_loss: 0.0784 - val_binary_accuracy: 0.1340\n",
      "Epoch 21/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0452 - binary_accuracy: 0.1341 - val_loss: 0.0768 - val_binary_accuracy: 0.1340\n",
      "Epoch 22/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0396 - binary_accuracy: 0.1341 - val_loss: 0.0747 - val_binary_accuracy: 0.1340\n",
      "Epoch 23/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0376 - binary_accuracy: 0.1341 - val_loss: 0.0747 - val_binary_accuracy: 0.1340\n",
      "Epoch 24/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0358 - binary_accuracy: 0.1341 - val_loss: 0.0737 - val_binary_accuracy: 0.1340\n",
      "Epoch 25/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0352 - binary_accuracy: 0.1341 - val_loss: 0.0706 - val_binary_accuracy: 0.1340\n",
      "Epoch 26/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0354 - binary_accuracy: 0.1341 - val_loss: 0.0696 - val_binary_accuracy: 0.1340\n",
      "Epoch 27/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0282 - binary_accuracy: 0.1341 - val_loss: 0.0697 - val_binary_accuracy: 0.1340\n",
      "Epoch 28/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0286 - binary_accuracy: 0.1341 - val_loss: 0.0692 - val_binary_accuracy: 0.1340\n",
      "Epoch 29/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0271 - binary_accuracy: 0.1341 - val_loss: 0.0683 - val_binary_accuracy: 0.1340\n",
      "Epoch 30/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0240 - binary_accuracy: 0.1341 - val_loss: 0.0682 - val_binary_accuracy: 0.1340\n",
      "Epoch 31/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0242 - binary_accuracy: 0.1341 - val_loss: 0.0676 - val_binary_accuracy: 0.1340\n",
      "Epoch 32/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0198 - binary_accuracy: 0.1341 - val_loss: 0.0678 - val_binary_accuracy: 0.1340\n",
      "Epoch 33/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0216 - binary_accuracy: 0.1341 - val_loss: 0.0670 - val_binary_accuracy: 0.1340\n",
      "Epoch 34/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0193 - binary_accuracy: 0.1341 - val_loss: 0.0677 - val_binary_accuracy: 0.1340\n",
      "Epoch 35/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0181 - binary_accuracy: 0.1341 - val_loss: 0.0677 - val_binary_accuracy: 0.1340\n",
      "Epoch 36/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0190 - binary_accuracy: 0.1341 - val_loss: 0.0679 - val_binary_accuracy: 0.1340\n",
      "Epoch 37/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0185 - binary_accuracy: 0.1341 - val_loss: 0.0665 - val_binary_accuracy: 0.1340\n",
      "Epoch 38/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0182 - binary_accuracy: 0.1341 - val_loss: 0.0670 - val_binary_accuracy: 0.1340\n",
      "Epoch 39/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0170 - binary_accuracy: 0.1341 - val_loss: 0.0667 - val_binary_accuracy: 0.1340\n",
      "Epoch 40/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0133 - binary_accuracy: 0.1341 - val_loss: 0.0677 - val_binary_accuracy: 0.1340\n",
      "Epoch 41/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0115 - binary_accuracy: 0.1341 - val_loss: 0.0683 - val_binary_accuracy: 0.1340\n",
      "Epoch 42/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0143 - binary_accuracy: 0.1341 - val_loss: 0.0672 - val_binary_accuracy: 0.1340\n",
      "Epoch 43/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0115 - binary_accuracy: 0.1341 - val_loss: 0.0689 - val_binary_accuracy: 0.1340\n",
      "Epoch 44/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0121 - binary_accuracy: 0.1341 - val_loss: 0.0680 - val_binary_accuracy: 0.1340\n",
      "Epoch 45/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0111 - binary_accuracy: 0.1341 - val_loss: 0.0692 - val_binary_accuracy: 0.1340\n",
      "Epoch 46/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0092 - binary_accuracy: 0.1341 - val_loss: 0.0695 - val_binary_accuracy: 0.1340\n",
      "Epoch 47/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0111 - binary_accuracy: 0.1341 - val_loss: 0.0702 - val_binary_accuracy: 0.1340\n",
      "Epoch 48/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0107 - binary_accuracy: 0.1341 - val_loss: 0.0707 - val_binary_accuracy: 0.1340\n",
      "Epoch 49/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0084 - binary_accuracy: 0.1341 - val_loss: 0.0717 - val_binary_accuracy: 0.1340\n",
      "Epoch 50/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0086 - binary_accuracy: 0.1341 - val_loss: 0.0719 - val_binary_accuracy: 0.1340\n",
      "Epoch 51/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0084 - binary_accuracy: 0.1341 - val_loss: 0.0724 - val_binary_accuracy: 0.1340\n",
      "Epoch 52/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0075 - binary_accuracy: 0.1341 - val_loss: 0.0743 - val_binary_accuracy: 0.1340\n",
      "Epoch 53/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0080 - binary_accuracy: 0.1341 - val_loss: 0.0741 - val_binary_accuracy: 0.1340\n",
      "Epoch 54/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0079 - binary_accuracy: 0.1341 - val_loss: 0.0747 - val_binary_accuracy: 0.1340\n",
      "Epoch 55/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0070 - binary_accuracy: 0.1341 - val_loss: 0.0755 - val_binary_accuracy: 0.1340\n",
      "Epoch 56/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0073 - binary_accuracy: 0.1341 - val_loss: 0.0762 - val_binary_accuracy: 0.1340\n",
      "Epoch 57/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0056 - binary_accuracy: 0.1341 - val_loss: 0.0781 - val_binary_accuracy: 0.1340\n",
      "Epoch 58/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0052 - binary_accuracy: 0.1341 - val_loss: 0.0789 - val_binary_accuracy: 0.1340\n",
      "Epoch 59/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0068 - binary_accuracy: 0.1341 - val_loss: 0.0789 - val_binary_accuracy: 0.1340\n",
      "Epoch 60/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0053 - binary_accuracy: 0.1341 - val_loss: 0.0804 - val_binary_accuracy: 0.1340\n",
      "Epoch 61/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0051 - binary_accuracy: 0.1341 - val_loss: 0.0818 - val_binary_accuracy: 0.1340\n",
      "Epoch 62/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0053 - binary_accuracy: 0.1341 - val_loss: 0.0824 - val_binary_accuracy: 0.1340\n",
      "Epoch 63/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0038 - binary_accuracy: 0.1341 - val_loss: 0.0840 - val_binary_accuracy: 0.1340\n",
      "Epoch 64/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0032 - binary_accuracy: 0.1341 - val_loss: 0.0852 - val_binary_accuracy: 0.1340\n",
      "Epoch 65/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0035 - binary_accuracy: 0.1341 - val_loss: 0.0863 - val_binary_accuracy: 0.1340\n",
      "Epoch 66/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0042 - binary_accuracy: 0.1341 - val_loss: 0.0890 - val_binary_accuracy: 0.1340\n",
      "Epoch 67/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0029 - binary_accuracy: 0.1341 - val_loss: 0.0903 - val_binary_accuracy: 0.1340\n",
      "Epoch 68/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0030 - binary_accuracy: 0.1341 - val_loss: 0.0894 - val_binary_accuracy: 0.1340\n",
      "Epoch 69/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0022 - binary_accuracy: 0.1341 - val_loss: 0.0923 - val_binary_accuracy: 0.1340\n",
      "Epoch 70/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0035 - binary_accuracy: 0.1341 - val_loss: 0.0921 - val_binary_accuracy: 0.1340\n",
      "Epoch 71/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0032 - binary_accuracy: 0.1341 - val_loss: 0.0934 - val_binary_accuracy: 0.1340\n",
      "Epoch 72/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0032 - binary_accuracy: 0.1341 - val_loss: 0.0967 - val_binary_accuracy: 0.1340\n",
      "Epoch 73/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0045 - binary_accuracy: 0.1341 - val_loss: 0.0967 - val_binary_accuracy: 0.1340\n",
      "Epoch 74/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0036 - binary_accuracy: 0.1341 - val_loss: 0.0981 - val_binary_accuracy: 0.1340\n",
      "Epoch 75/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0027 - binary_accuracy: 0.1341 - val_loss: 0.0988 - val_binary_accuracy: 0.1340\n",
      "Epoch 76/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0035 - binary_accuracy: 0.1341 - val_loss: 0.1012 - val_binary_accuracy: 0.1340\n",
      "Epoch 77/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0031 - binary_accuracy: 0.1341 - val_loss: 0.1024 - val_binary_accuracy: 0.1340\n",
      "Epoch 78/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0031 - binary_accuracy: 0.1341 - val_loss: 0.1037 - val_binary_accuracy: 0.1340\n",
      "Epoch 79/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0018 - binary_accuracy: 0.1344 - val_loss: 0.1061 - val_binary_accuracy: 0.1340\n",
      "Epoch 80/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0025 - binary_accuracy: 0.1346 - val_loss: 0.1113 - val_binary_accuracy: 0.1340\n",
      "Epoch 81/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0022 - binary_accuracy: 0.1344 - val_loss: 0.1119 - val_binary_accuracy: 0.1340\n",
      "Epoch 82/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0030 - binary_accuracy: 0.1349 - val_loss: 0.1105 - val_binary_accuracy: 0.1340\n",
      "Epoch 83/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0028 - binary_accuracy: 0.1341 - val_loss: 0.1102 - val_binary_accuracy: 0.1340\n",
      "Epoch 84/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0018 - binary_accuracy: 0.1341 - val_loss: 0.1140 - val_binary_accuracy: 0.1340\n",
      "Epoch 85/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0023 - binary_accuracy: 0.1351 - val_loss: 0.1149 - val_binary_accuracy: 0.1340\n",
      "Epoch 86/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0015 - binary_accuracy: 0.1346 - val_loss: 0.1164 - val_binary_accuracy: 0.1340\n",
      "Epoch 87/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0027 - binary_accuracy: 0.1341 - val_loss: 0.1149 - val_binary_accuracy: 0.1340\n",
      "Epoch 88/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0018 - binary_accuracy: 0.1351 - val_loss: 0.1182 - val_binary_accuracy: 0.1340\n",
      "Epoch 89/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0013 - binary_accuracy: 0.1354 - val_loss: 0.1209 - val_binary_accuracy: 0.1340\n",
      "Epoch 90/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0016 - binary_accuracy: 0.1349 - val_loss: 0.1216 - val_binary_accuracy: 0.1340\n",
      "Epoch 91/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0020 - binary_accuracy: 0.1349 - val_loss: 0.1228 - val_binary_accuracy: 0.1340\n",
      "Epoch 92/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 7.3856e-04 - binary_accuracy: 0.1351 - val_loss: 0.1263 - val_binary_accuracy: 0.1340\n",
      "Epoch 93/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0017 - binary_accuracy: 0.1359 - val_loss: 0.1264 - val_binary_accuracy: 0.1340\n",
      "Epoch 94/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 6.8385e-04 - binary_accuracy: 0.1369 - val_loss: 0.1300 - val_binary_accuracy: 0.1340\n",
      "Epoch 95/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0023 - binary_accuracy: 0.1372 - val_loss: 0.1339 - val_binary_accuracy: 0.1340\n",
      "Epoch 96/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0016 - binary_accuracy: 0.1374 - val_loss: 0.1306 - val_binary_accuracy: 0.1340\n",
      "Epoch 97/100\n",
      "122/122 [==============================] - 0s 2ms/step - loss: 0.0013 - binary_accuracy: 0.1362 - val_loss: 0.1311 - val_binary_accuracy: 0.1340\n",
      "Epoch 98/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 5.9446e-04 - binary_accuracy: 0.1359 - val_loss: 0.1353 - val_binary_accuracy: 0.1340\n",
      "Epoch 99/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 0.0021 - binary_accuracy: 0.1385 - val_loss: 0.1358 - val_binary_accuracy: 0.1340\n",
      "Epoch 100/100\n",
      "122/122 [==============================] - 0s 1ms/step - loss: 7.2539e-04 - binary_accuracy: 0.1390 - val_loss: 0.1368 - val_binary_accuracy: 0.1340\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "history = model.fit(\n",
    "    x=X_train,y=y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 0s 625us/step - loss: 0.1368 - binary_accuracy: 0.1340\n",
      "0.13683274388313293\n",
      "0.13397128880023956\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(loss)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1440    8]\n",
      " [  21  203]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]], dtype=int32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes([\"How are you doing\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: tf_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('tf_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function recreate_function.<locals>.restored_function_body at 0x7ff856619790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function recreate_function.<locals>.restored_function_body at 0x7ff857d59c10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Cannot convert a Tensor of dtype resource to a NumPy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-c2d668f1cfc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Convert the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mconverter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf_model/\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# path to the SavedModel directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtflite_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Save the model.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model.tflite'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/kpmg_intership_1/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m         \u001b[0mInvalid\u001b[0m \u001b[0mquantization\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1075\u001b[0m     \"\"\"\n\u001b[0;32m-> 1076\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFLiteConverterV2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/kpmg_intership_1/lib/python3.8/site-packages/tensorflow/lite/python/lite.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m     frozen_func, graph_def = (\n\u001b[0;32m--> 877\u001b[0;31m         _convert_to_constants.convert_variables_to_constants_v2_as_graph(\n\u001b[0m\u001b[1;32m    878\u001b[0m             self._funcs[0], lower_control_flow=False))\n\u001b[1;32m    879\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/kpmg_intership_1/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py\u001b[0m in \u001b[0;36mconvert_variables_to_constants_v2_as_graph\u001b[0;34m(func, lower_control_flow, aggressive_inlining)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0mtransformations\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfrozen\u001b[0m \u001b[0mphase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m   \"\"\"\n\u001b[0;32m-> 1100\u001b[0;31m   converter_data = _FunctionConverterData(\n\u001b[0m\u001b[1;32m   1101\u001b[0m       \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mlower_control_flow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlower_control_flow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/kpmg_intership_1/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, lower_control_flow, aggressive_inlining, variable_names_whitelist, variable_names_blacklist)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0mvariable_names_whitelist\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvariable_names_whitelist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m         variable_names_blacklist=variable_names_blacklist)\n\u001b[0;32m--> 804\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_tensor_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_build_tensor_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/kpmg_intership_1/lib/python3.8/site-packages/tensorflow/python/framework/convert_to_constants.py\u001b[0m in \u001b[0;36m_build_tensor_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_index_to_variable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    824\u001b[0m       self._tensor_data[tensor_name] = _TensorData(\n\u001b[1;32m    825\u001b[0m           \u001b[0mnumpy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/kpmg_intership_1/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1061\u001b[0m     \"\"\"\n\u001b[1;32m   1062\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/kpmg_intership_1/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1029\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/kpmg_intership_1/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Cannot convert a Tensor of dtype resource to a NumPy array."
     ]
    }
   ],
   "source": [
    "# Convert the model\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(\"tf_model/\") # path to the SavedModel directory\n",
    "tflite_model = converter.convert()\n",
    "# Save the model.\n",
    "with open('model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('kpmg_intership_1': conda)",
   "language": "python",
   "name": "python38364bitkpmgintership1condab5373946c1054560a1cd9f27bc604b47"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
